{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00260df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Summarization Inference and Testing\n",
    "# This notebook loads a trained model and generates summaries for test documents\n",
    "\n",
    "## Import Required Libraries\n",
    "\n",
    "import os\n",
    "import logging\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Configure TensorFlow logging to only show errors\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "\n",
    "# Disable tokenizer parallelism warnings\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467cf503",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Configuration Parameters\n",
    "\n",
    "# Inference Configuration\n",
    "MAX_INPUT_LENGTH = 1024    # Maximum length of input text (tokens)\n",
    "MIN_TARGET_LENGTH = 5      # Minimum length of generated summary (tokens)\n",
    "MAX_TARGET_LENGTH = 128    # Maximum length of generated summary (tokens)\n",
    "# Alternative: MAX_TARGET_LENGTH = 400  # For longer summaries\n",
    "\n",
    "# Model Selection\n",
    "# Specify the model checkpoint that matches your trained model\n",
    "# MODEL_CHECKPOINT = \"t5-small\"              # For T5 models\n",
    "MODEL_CHECKPOINT = \"facebook/bart-base\"      # For BART models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfd1dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at ./t5_small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "## Load Trained Model\n",
    "\n",
    "# Load the fine-tuned model from a local directory\n",
    "# Make sure the path points to your saved model directory\n",
    "from transformers import TFAutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
    "\n",
    "model_path = \"./t5_small\"  # Update this path to your trained model location\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "print(f\"Model loaded from: {model_path}\")\n",
    "print(f\"Model type: {type(model).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7785ff6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "## Compile Model (Optional for Inference)\n",
    "\n",
    "# Compiling the model is not strictly necessary for inference,\n",
    "# but it ensures the model is ready if you want to continue training\n",
    "optimizer = keras.optimizers.Adam(learning_rate=2e-5)\n",
    "model.compile(optimizer=optimizer)\n",
    "print(\"Model compiled successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270aa56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Tokenizer\n",
    "\n",
    "# Load the tokenizer corresponding to the model architecture\n",
    "# The tokenizer must match the model (T5 or BART)\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "print(f\"Tokenizer loaded: {MODEL_CHECKPOINT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6064c6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Configure Model-Specific Prefix\n",
    "\n",
    "# T5 models require a task prefix for text-to-text tasks\n",
    "# BART models don't need a prefix\n",
    "if MODEL_CHECKPOINT in [\"t5-small\", \"t5-base\", \"t5-large\", \"t5-3b\", \"t5-11b\"]:\n",
    "    prefix = \"summarize: \"  # T5 prefix for summarization task\n",
    "else:\n",
    "    prefix = \"\"  # No prefix for BART models\n",
    "\n",
    "print(f\"Using prefix: '{prefix}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "435479bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example = \"Fresh shelling from Ukraine rocked Belgorod overnight, the governor said in a video posted Sunday morning, as Russian dissidents ramp up pressure on the western border region.Gov. Vyacheslav Gladkov said there had been Ukrainian attacks in several locations under his administration.`The night was rather turbulent,` Gladkov said. `There is a lot of destruction. There is no information about casualties.`Due to the violence, 4,000 people are being housed in temporary accommodations. Children in the area are being moved to a camp in Crimea for their own safety, Gladkov added.Dissidents appear near shelled area: Also Sunday, the Freedom for Russia Legion, one of two dissident Russian units fighting under Ukrainian command, posted a video which they said showed their fighters on the streets of a village on the outskirts of Shebekino, one of the areas Gladkov said was attacked.The footage appeared to show the legion in Novaya Tavolzhanka, according to geolocation by CNN, and groups of people moving through the streets as a unit.“We’re going in! The advance assault group of the Legion and the Russian Volunteer Corp entering the suburb of Shebekino,” the group said in the clip's caption.CNN cannot verify the legion’s claim, but the video’s release will be seen as a further attempt to destabilize Russia in the information space as well as disrupting its military plans.Meetings requested: In another bold move, the legion posted a video in which its leader and that of a second dissident group, the Russian Volunteer Corps, request a meeting with Gladkov. In exchange, they offered to release two Russian soldiers allegedly in their custody.The video shows the purported soldiers giving their names and those of their hometowns in Russia. The dissident leaders -- who have made no secret of their opposition to Russian President Vladimir Putin -- say they want to talk to Gladkov about the fate of the country and the war. No threat is made to the lives of the men they are holding.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e9a07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add Prefix to Input (if needed)\n",
    "\n",
    "# Add the task prefix for T5 models (BART models don't need this)\n",
    "example_pref = prefix + example\n",
    "print(f\"Input text with prefix: {len(example_pref)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908fb70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenize Input Text\n",
    "\n",
    "# Convert the input text to token IDs that the model can process\n",
    "model_input = tokenizer(\n",
    "    example_pref, \n",
    "    max_length=MAX_INPUT_LENGTH, \n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "print(f\"Tokenized input length: {len(model_input['input_ids'])} tokens\")\n",
    "print(f\"Input IDs shape: {model_input['input_ids']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a9fc10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': \"MG Rover's proposed tie-up with Chinese carmaker Shanghai Automotive could result in 3,000 jobs being lost if the deal goes ahead, according to the Financial Times.\"}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Generate Summary\n",
    "\n",
    "# Create a summarization pipeline for easy inference\n",
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\n",
    "    \"summarization\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    framework=\"tf\"\n",
    ")\n",
    "\n",
    "# Generate summary with specified length constraints\n",
    "print(\"Generating summary...\")\n",
    "print(\"=\"*50)\n",
    "result = summarizer(\n",
    "    example,\n",
    "    min_length=MIN_TARGET_LENGTH,\n",
    "    max_length=MAX_TARGET_LENGTH,\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"\\nGenerated Summary:\")\n",
    "print(\"-\" * 50)\n",
    "print(result[0]['summary_text'])\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"\\nOriginal Text (first 300 characters):\")\n",
    "print(\"-\" * 50)\n",
    "print(example[:300] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58c7791",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6578fc65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
